# ğŸš€ RunPod Llama-3.1-8B Fine-tuning Rehberi

Bu rehber, RunPod Ã¼zerinde Llama-3.1-8B-Instruct modelini TÃ¼rkÃ§e dating conversation verisi ile fine-tune etmek iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.

## ğŸ¯ **Llama-3.1-8B AvantajlarÄ±**

### âœ¨ **Performans**
- **Daha HÄ±zlÄ±**: 8B parametre ile Ã§ok daha hÄ±zlÄ± eÄŸitim
- **Daha Az VRAM**: 10-16GB VRAM yeterli
- **Daha HÄ±zlÄ± Inference**: ~200-300 tokens/s
- **Daha Az Maliyet**: RunPod'da daha ucuz GPU'lar kullanÄ±labilir

### ğŸ”§ **Teknik Ã–zellikler**
- **Model Size**: ~16GB (base) + ~100MB (LoRA)
- **Training Time**: ~2-4 saat (3 epoch)
- **Memory Usage**: ~12-16GB VRAM
- **Batch Size**: 2 (gradient accumulation: 4)

## ğŸ“‹ **Gereksinimler**

### DonanÄ±m Gereksinimleri
- **GPU**: RTX 3080 (10GB) - minimum
- **RAM**: 16GB sistem RAM
- **Disk**: 50GB boÅŸ alan
- **CPU**: 4+ core

### RunPod Template
- **Docker Image**: `runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04`
- **Container Disk**: 50GB
- **Volume**: 25GB
- **Ports**: 8000, 8888

## ğŸš€ **HÄ±zlÄ± BaÅŸlangÄ±Ã§**

### 1. **RunPod'a BaÄŸlan**
```bash
# RunPod terminal'de
cd /workspace
```

### 2. **HÄ±zlÄ± Setup**
```bash
# Setup script Ã§alÄ±ÅŸtÄ±r
bash runpod_quick_start.sh
```

### 3. **Fine-tuning BaÅŸlat**
```bash
# Training baÅŸlat
python runpod_simple_finetune.py
```

### 4. **Model Test**
```bash
# Test et
python test_llama_8b_model.py
```

## âš™ï¸ **KonfigÃ¼rasyon**

### Model Settings
```python
model_name = "meta-llama/Llama-3.1-8B-Instruct"
output_dir = "./llama_8b_dating_model"
```

### LoRA Settings (8B iÃ§in optimize)
```python
lora_r = 32          # 70B'de 64'tÃ¼
lora_alpha = 64      # 70B'de 128'di
lora_dropout = 0.1
```

### Training Settings
```python
per_device_train_batch_size = 2      # 70B'de 1'di
gradient_accumulation_steps = 4      # 70B'de 8'di
learning_rate = 3e-4                 # 70B'de 2e-4'tÃ¼
```

## ğŸ“Š **Performance KarÅŸÄ±laÅŸtÄ±rmasÄ±**

| Ã–zellik | Llama-3.3-70B | Llama-3.1-8B |
|---------|---------------|---------------|
| **VRAM** | 24-40GB | 10-16GB |
| **Training Time** | 6-12 saat | 2-4 saat |
| **Inference Speed** | 50-100 tokens/s | 200-300 tokens/s |
| **Model Size** | 40GB + 200MB | 16GB + 100MB |
| **Batch Size** | 1 | 2 |
| **RunPod Cost** | YÃ¼ksek | DÃ¼ÅŸÃ¼k |

## ğŸ¯ **RunPod Terminal KomutlarÄ±**

```bash
# 1. Setup
bash runpod_quick_start.sh

# 2. Training baÅŸlat
python runpod_simple_finetune.py

# 3. Background'da Ã§alÄ±ÅŸtÄ±r
nohup python runpod_simple_finetune.py > training.log 2>&1 &

# 4. Progress takip et
tail -f training.log

# 5. Test et
python test_llama_8b_model.py
```

## ğŸ“ˆ **Monitoring**

```bash
# GPU kullanÄ±mÄ±
nvidia-smi -l 1

# Training progress
tail -f training.log

# Weights & Biases (browser'da)
# Otomatik olarak link oluÅŸturulur
```

## ğŸ” **Environment Variables**

```bash
HUGGING_FACE_HUB_TOKEN=your_token_here
WANDB_API_KEY=your_wandb_key_here
```

## ğŸ“ **Proje YapÄ±sÄ±**

```
/workspace/
â”œâ”€â”€ runpod_simple_finetune.py    # Ana fine-tuning script
â”œâ”€â”€ test_llama_8b_model.py       # Model test script
â”œâ”€â”€ runpod_quick_start.sh        # Setup script
â”œâ”€â”€ requirements_simple.txt      # Dependencies
â”œâ”€â”€ runpod_config.yaml          # RunPod config
â”œâ”€â”€ llama_8b_dating_model/      # Fine-tuned model
â””â”€â”€ training.log                # Training logs
```

## ğŸ§ª **Test Ã–rnekleri**

```python
test_messages = [
    "Merhaba, nasÄ±lsÄ±n?",
    "KaÃ§ yaÅŸÄ±ndasÄ±n?",
    "Nerede yaÅŸÄ±yorsun?",
    "Seni tanÄ±mak istiyorum",
    "Kahve iÃ§meye ne dersin?",
    "Hangi mÃ¼zik tÃ¼rlerini seviyorsun?",
    "En sevdiÄŸin film nedir?",
    "Hangi ÅŸehirde yaÅŸamak istersin?",
    "Hobilerin neler?",
    "NasÄ±l bir insan arÄ±yorsun?"
]
```

## ğŸš¨ **Sorun Giderme**

### 1. **CUDA Out of Memory**
```bash
# Batch size azalt
per_device_train_batch_size = 1

# Gradient accumulation artÄ±r
gradient_accumulation_steps = 8
```

### 2. **Slow Training**
```bash
# GPU kullanÄ±mÄ±nÄ± kontrol et
nvidia-smi

# DataLoader optimizasyonu
dataloader_pin_memory = False
```

### 3. **Model YÃ¼klenemiyor**
```bash
# Hugging Face token kontrol
huggingface-cli whoami

# Disk alanÄ± kontrol
df -h
```

## ğŸ¯ **KullanÄ±m SenaryolarÄ±**

### 1. **HÄ±zlÄ± Test**
```bash
python runpod_simple_finetune.py && python test_llama_8b_model.py
```

### 2. **Production Deployment**
```bash
# Training
python runpod_simple_finetune.py

# Deploy (Flask API)
python deploy_llama_model.py --host 0.0.0.0 --port 8000
```

### 3. **Interactive Development**
```bash
# Jupyter notebook
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

## ğŸ“š **Kaynaklar**

- [Llama-3.1-8B Documentation](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)
- [RunPod Documentation](https://docs.runpod.io/)
- [PEFT Documentation](https://huggingface.co/docs/peft/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)

## ğŸ¤ **Destek**

Sorun yaÅŸÄ±yorsanÄ±z:

1. **GitHub Issues**: Proje repository'sinde issue aÃ§Ä±n
2. **RunPod Support**: RunPod support ekibiyle iletiÅŸime geÃ§in
3. **Community**: Hugging Face community'de soru sorun

---

**Not**: Llama-3.1-8B, 70B modeline gÃ¶re Ã§ok daha verimli ve maliyet-etkin bir seÃ§imdir. AynÄ± kalitede sonuÃ§lar alÄ±rken Ã§ok daha hÄ±zlÄ± eÄŸitilir ve daha az kaynak kullanÄ±r.
